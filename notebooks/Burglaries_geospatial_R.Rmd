---
title: "Burgalry Incidents in Davidson County, TN"
output: html_notebook
---

Part 1


```{r}
#install.packages("sf")
#install.packages("leaflet")
#install.packages("lme4")
#install.packages("AER")
#install.packages("MASS")
library(sf)
library(tidyverse) 
library(lme4)
```

Let's load in our zipcode dataset. This is a geojson file, which was downloaded from <https://data.nashville.gov/Metro-Government/Zip-Codes/72um-tmbe>. Geosjson is a standard, based on JSON, for representing geographic features.

```{r}
CensusTracts <- st_read('../data/DC')
```

Notice that the ouput provides some information about the data we just read in. It contains multipolygons and is using WGS 84 as the CRS, or coordinate reference system, which defines how the points on the globe are mapped to the 2-dimensional projection we are working with.

The actual object we read in is a simple feature collection, which means that it acts like a data frame, but it also contains additional geographic information.

```{r}
CensusTracts
```

We can create plots out of our geospatial data using ggplot.

```{r}
CensusTracts |> 
  ggplot() +
  geom_sf() 
```

We can adjust the color using the fill aesthethic.

```{r}
# CensusTracts |> 
#   ggplot() +
#   geom_sf(aes(fill = TRACTCE))
```

We might also want to work with data in the form a csv file.

```{r}
Burglaries2023 <- read_csv("../data/burglaries_2023.csv")
Burglaries2023
```



```{r}

Burglaries2023 <- Burglaries2023 |> 
  distinct(incident_number, .keep_all = TRUE) |> 
  drop_na(latitude) 
  # |> 
  # select(incident_number, latitude, longitude)   

Burglaries2023 

```




Let's extract the latitude and longitude using the str_extract function from stringr.

```{r}
# Burglaries2023 <- Burglaries2023 |> 
#   mutate(
#     latitude = as.numeric(str_extract(`Mapped Location`, "\\((.+),", group = 1)),
#     longitude = as.numeric(str_extract(`Mapped Location`, ",(.+)\\)", group = 1))
#     )
```

Now, we can plot these burglary incidents. Note that I'll start with the census tract plot and then add in the burglary incidents by using geom_point.

```{r}
CensusTracts |> 
  ggplot() +
  geom_sf() +
  geom_point(
    data = Burglaries2023 |> drop_na(latitude),
    aes(x = longitude, y = latitude),
    size = 0.1
    )
```

It appears that some of the burglary incidents are outside of Davidson County. What if we want to select only those inside the county?

If we want to be able to perform geospatial operations on our bus stops, we need to convert it into a geospatial object. We can do this using the st_as_sf function.

```{r}

Burglaries2023_geo <- st_as_sf(
  Burglaries2023 |> drop_na(latitude),
  coords = c('longitude', 'latitude'),
  crs = st_crs(CensusTracts)
)

Burglaries2023_geo

```

side note / clarification: crs = st_crs(CensusTracts) is using the CensusTracts' metadata to set up projection, not using the geo coordinates within the shape files. 



Now, to build our plot, we can use geom_sf.

```{r}
CensusTracts |> 
  ggplot() +
  geom_sf() +
  geom_sf(data = Burglaries2023_geo, size = 0.1)
```

To select only the bus stops inside the County, we can perform a spatial join. The sf library has an st_join function. We need to specify how to join (contains, intersects, etc.). Here, we'll use containment (which is st_within; plot bus stop as long as it's within zipcode). 

st_contains is left join
 


```{r}
# Burglaries2023_CensusTracts <- st_join(Burglaries2023_geo, CensusTracts, join = st_within)  
# Burglaries2023_CensusTracts
```  

Alternatively, we can narrow down to incidents that are within Davidson County.
CRS = Coordinate Ref System


```{r}
Burglaries2023_CensusTracts <- st_join(Burglaries2023_geo, CensusTracts, join = st_within, left = FALSE)  
Burglaries2023_CensusTracts
```  

side note: in this case / step, we are using the geo coordinates in CensusTracts



```{r}

Burglaries2023_CensusTracts |> 
                                filter(is.na(incident_number))  

```









Plotting of Burglary Incidents within Davidson County:
```{r}

CensusTracts |> 
  ggplot() +
  geom_sf() +
  geom_sf(data = Burglaries2023_CensusTracts, size = 0.1)

```


Notice that bus_zips retains the geometry of bus_stops, as this was the first dataframe we provided. Now, we can, for example, count the number of stops per zipcode. Note that when performing calculations on an sf object, it is usually quicker if you drop the geometry first.

```{r}
Burglaries2023_CensusTracts |> 
  filter(is.na(incident_number))
```


```{r}
Burglaries2023_CensusTracts |> 
  st_drop_geometry() |> 
  group_by(GEOID) |> 
  count(name = "incident_number") |> 
  arrange(desc(incident_number))
```

Now, we can plot the bus stops contained in a given zipcode.

```{r}
# zip = 37207
# 
# zipcodes |> 
#   filter(zipcode == zip) |> 
#   ggplot() +
#   geom_sf() +
#   geom_sf(data = bus_zips |> filter(zipcode == zip),
#           aes(color = `Route Name`))
```

Finally, if we want to make an interactive map, we can use the leaflet library (<https://rstudio.github.io/leaflet/>).

```{r}
library(leaflet)
```

```{r}
leaflet(data = Burglaries2023 |> 
          drop_na(latitude) # |> 
     #     filter(`Route Name` == "WEST END - WHITE BRIDGE")
        ) |>  
  addTiles() |> 
  addMarkers(~longitude, 
             ~latitude, 
             popup = ~as.character(`incident_number`), 
             label = ~as.character(`incident_number`)
             )
```

You can also do marker clusters.

```{r}
leaflet(Burglaries2023 |> drop_na(longitude)) %>% 
  addTiles() %>% 
  addMarkers(
    ~longitude,
    ~latitude,
    clusterOptions = markerClusterOptions(),
    popup = ~as.character(`incident_number`), 
    label = ~as.character(`incident_number`)
  )
```

Part 2 -


 

Read in the CSV that contains data on Median Income and Population from Census:

```{r}
census <- read_csv("../data/census.csv")
census
```

Preparing for inner join by 
(1) making the column headers of primary key congruent:

```{r}

Burglaries2023_CensusTracts <- Burglaries2023_CensusTracts  |>  
  group_by(TRACTCE) |> 
  summarize(num_burglaries = n_distinct(incident_number, na.rm = TRUE)) |> 
  arrange(desc(num_burglaries))  |>  
  rename(tract = TRACTCE)  |>  
  right_join(census)

Burglaries2023_CensusTracts

```



EDA: initial exploration ~


Let's look at the distributions of population size by tract, since tracts with larger population may have more burglaries. 

```{r}
Burglaries2023_CensusTracts |> 
  ggplot(aes(x = population)) +
  geom_histogram()
```

Let's also peruse the distribution of median income: 

```{r}
Burglaries2023_CensusTracts |> 
  ggplot(aes(x = median_income)) +
  geom_histogram()
```


```{r}
Burglaries2023_CensusTracts  |> 
  arrange(population)
```

We need to drop rows with negative median income:

```{r}

Burglaries2023_CensusTracts <- Burglaries2023_CensusTracts  |> 
  filter(population > 0, median_income > 0)

```




```{r}
 
Burglaries2023_CensusTracts |> 
  ggplot(aes(x = median_income)) +
  geom_histogram() 

```





```{r}

Burglaries2023_CensusTracts |> 
  ggplot(aes(x = num_burglaries)) +
  geom_histogram()

```


```{r}
Burglaries2023_CensusTracts |> 
  ggplot(aes(x = median_income, y = num_burglaries)) +
  geom_point()
```


And on a per thousand population basis.

```{r}
Burglaries2023_CensusTracts  |>  
  mutate(burglaries_per_thousand = num_burglaries / (population/1000)) |> 
  ggplot(aes(x = median_income, y = burglaries_per_thousand)) +
  geom_point()
```



Aggregate data by census tract. Which census tract had the highest number of burglaries? 

Ans: Census Tract 016000, which has 39 burglaries 

```{r}
Burglaries2023_CensusTracts |> 
                    arrange(desc(num_burglaries)) |> 
                    head(1)
```



 
Which census tract had the highest number of burglaries per 1000 residents?

Ans: Tract 016000 has the Incidence Proportion of 15.1750973 incidents per 1000 residents. 


```{r}

IncidenceProportion_byTract <- Burglaries2023_CensusTracts |> 
                                  mutate(Incidents_Per_1000_Residents = num_burglaries / (population/1000)) |> 
                                  arrange(desc(Incidents_Per_1000_Residents)) |> 
                                  filter(Incidents_Per_1000_Residents != "Inf") 

IncidenceProportion_byTract 

```

```{r}

# IncidenceCountPer1000_DeDup_byTract <- IncidenceCountPer1000_byTract[!duplicated(IncidenceCountPer1000_byTract[c('Incidents_Per_1000_Residents')]), ]
# IncidenceCountPer1000_DeDup_byTract
# 
# 


IncidenceProportion_byTract_ArrangeByMedIncome <- IncidenceProportion_byTract   |> 
                                                     arrange(median_income)  
                                           
IncidenceProportion_byTract_ArrangeByMedIncome

```


Finally, look at the relationship between median income and number of aggravated burglaries per tract. How would you describe this relationship?

Ans: For every additional dollar in a census tract's Median Income, we see a decrease of -2.385e-05 burglary incidents per 1000 people (Incidence Proportion). 





```{r}

lr_MedIncome_IncidenceProportion <- glm(Incidents_Per_1000_Residents ~ median_income, family = gaussian(link = "identity"), data = IncidenceProportion_byTract_ArrangeByMedIncome)

summary(lr_MedIncome_IncidenceProportion)

```


Bonus Question:

Fit a Poisson regression model with target variable the rate of burglaries per census tract and with predictor the median income. Offset using the log of the population so that we are looking at the rate of burglaries per population instead of the number of burglaries. How can you interpret the meaning of the output? How do the estimates from the model compare to the observed data?


Steps -

1) take a natural log of population count
2) divide incident count by natural log of population count



### Regression Models

   

```{r}
 

Poisson_RegModel_IncidenceProp_corr_MedIncome <- glm(num_burglaries ~ median_income + population + population:median_income, family = "poisson", offset = log(population), data = IncidenceProportion_byTract_ArrangeByMedIncome)

summary(Poisson_RegModel_IncidenceProp_corr_MedIncome)

```
 
 
Reduced model of examining the association between burglary incidence proportion and median income:


```{r}
burg_poisson <- glm(num_burglaries ~ median_income + offset(log(population)), 
                    family = poisson, 
                    data =  IncidenceProportion_byTract_ArrangeByMedIncome)

summary(burg_poisson)
```

When Median Income = $40000
and Population = 5000
10.58664 Burglary Incidents tend to take place.  
```{r}
population = 5000
exp(-5.184 + log(population) + 40000 * -2.434e-05)
```
  
```{r}
median_income <- seq(from = 20000, to = 180000, length.out = 5)
population = 1000

map(median_income, 
    \(x) tibble(median_income = x, 
                num_burglaries = 0:20, 
                probability = dpois(0:20, 
                                      lambda = predict(Poisson_RegModel_IncidenceProp_corr_MedIncome,
                                                   newdata = tibble(median_income = x, population = population), type = "response")
                                    )
                )
    ) |> 
  bind_rows() |> 
  ggplot(aes(x = num_burglaries, y = probability)) +
  geom_col() +
  facet_wrap(~median_income)
```




```{r}
library(AER)

dispersiontest(burg_poisson)
```
Extra: What about overdispersion? (That is, does the response really follow a Poisson distribution?)

What could go wrong? If the model has overdispersion, our standard errors will be too small, which increases the chances of a Type I error.

A good explanation is available in [this video](https://www.youtube.com/watch?v=uGKnoAw-PFQ). 



```{r}

Poisson_RegModel_IncidenceProp_corr_MedIncome_QP <- glm(num_burglaries ~ median_income + population + population:median_income, family = "quasipoisson", offset = log(population), data = IncidenceProportion_byTract_ArrangeByMedIncome)

summary(Poisson_RegModel_IncidenceProp_corr_MedIncome_QP)

```


```{r} 

library(MASS)

Poisson_RegModel_IncidenceProp_corr_MedIncome_NB <- glm.nb(num_burglaries ~ median_income + population + population:median_income, offset(log(population)), data = IncidenceProportion_byTract_ArrangeByMedIncome)

summary(Poisson_RegModel_IncidenceProp_corr_MedIncome_NB)

```


```{r}

Poisson_RegModel_IncidenceProp_corr_MedIncome_NB$theta

```

 
```{r} 


median_income <- seq(from = 20000, to = 180000, length.out = 5)
population = 1000

mu <- predict(Poisson_RegModel_IncidenceProp_corr_MedIncome_NB, 
              newdata = tibble(median_income = median_income,
                               population = population), 
              type = "response")

var <- mu + mu^2 / Poisson_RegModel_IncidenceProp_corr_MedIncome_NB$theta

map(median_income, 
    \(x) tibble(median_income = x, 
                num_burglaries = 0:20, 
                probability = dnbinom(0:20, 
                                      mu = predict(Poisson_RegModel_IncidenceProp_corr_MedIncome_NB,
                                                   newdata = tibble(median_income = x, population = population), type = "response"),
                                      size = Poisson_RegModel_IncidenceProp_corr_MedIncome_NB$theta
                                    )
                )
    ) |> 
  bind_rows() |> 
  ggplot(aes(x = num_burglaries, y = probability)) +
  geom_col() +
  facet_wrap(~median_income)


``` 

 





Helpful footnote on Poisson Regression:
https://generalizedregressionmodelingagency.github.io/GRMA/poisson-regression.html

"Poisson family can be checked for overdispersion by comparing the residual deviance to the residual degrees of freedom. If the residual deviance is greater than the degrees of freedom, the model is overdispersed, and if the residual deviance is much less than the degrees of freedom, the model is underdispersed, a less frequent occurence. A large residual deviance compared to the residual degrees of freedom could also indicate a lack of fit, but this can be checked by eliminating outliers and fitting the model with the most explanatory variables possible. The residual deviance will still be large if overdispersion is the issue. Similarly, the Pearson goodness-of-fit statistic can be compared to the residual degrees of freedom to check for overdispersion. If the counts are small, so asymptotic approximations might not be accurate, large goodness-of-fit statistics generally indicate a poor model fit."


